---
title: "ETL Process and Challenge"
date: 2020-xx-xx
tags: [Data Science]
excerpt: "Data Cleaning & ETL"
mathjax: "true"
---

# Project Overview
The goal of this project is to show my process in preforming ETL on a new dataset and how to find errors we would want to remove before preforming any analysis. The dataset was part of a challenge through SuperDataScience, a platform that teaches data science.

The dataset is just over 1M rows and has 5 errors that we need to try to find. 'describe the dataset further'...

Link to Dataset: [Data]()

Link to GitHub Repo: [GitHub]()

Link to Tableau Visualizations: [Visualization]()

## Data Cleaning
First we import our libaries, like Pandas, NumPy, and Seaborn, with some other parameters. Next we import the data into Jupyter Notebooks and display the data to see what they provide.

### EDA Lite
I wanted to answer a small question bugging me first. Specifically on why they decided to seperate the files based on the states they used. For example, why only put 8 states in eo1.csv and why 24 states in eo3.csv? Could it be based on some metric? Let's find out.

### Data Cleaning Part II
Now we will seperate the Top 10 Non-Profits per State based on Income, merge them, and export the new dataset for further EDA in Tableau.

## Exploratory Data Analysis (EDA)

## Final Thoughts
I really appricate anyone who took the time to read through. I'm mainly building these posts as a way to reteach myself the tools and techniques I have been using and learning the past year and half. Hopefully with the help of the Protégé Effect where by teaching, or even pretending to teach, information to others helps that person learn the information. I'm looking forward to making more complex projects and solving business problems. Thank you again!
